<!DOCTYPE html>
<html data-wf-domain="peizhuoli.github.io" data-wf-page="5e6fb768456f961381500a5f"
  data-wf-site="51e0d73d83d06baa7a00000f">

<head>
  <meta charset="utf-8" />
  <title>WalkTheDog</title>
  <meta content="summary" name="twitter:card" />
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="./walkthedog.css" rel="stylesheet" type="text/css" />
  <script>
    window.MathJax = {
      tex: {
        macros: {
          R: "\\mathbb{R}",
          exp: "\\mathrm{e}^{#1}",
          X: "{\\bf X}",
          A: "{\\bf A}",
          P: "{\\bf P}",
        }
      },
      startup: {
        pageReady: function () {
          return MathJax.startup.defaultPageReady();
        }
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script
    type="text/javascript">WebFont.load({ google: { families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"] } });</script>
  <script
    type="text/javascript">!function (o, c) { var n = c.documentElement, t = " w-mod-"; n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch") }(window, document);</script>
  <link href="resources/logo.jpg" rel="shortcut icon" type="image/x-icon" />
  <link href="resources/logo.jpg" rel="apple-touch-icon" />
</head>

<body>
  <div class="section hero nerf-_v2">
    <div class="container-2 nerf_header_v2 w-container">
      <h1 class="nerf_title_v2">WalkTheDog: Cross-Morphology Motion Alignment
        via Phase Manifolds</h1>
      <h1 class="nerf_subheader_v2">SIGGRAPH 2024</h1>
      <div class="nerf_authors_list_single w-row">
        <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="https://peizhuoli.github.io/" target="_blank"
            class="nerf_authors_v2">Peizhuo Li</a></div>
        <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6"><a href="https://www.sebastianxstarke.com/"
            target="_blank" class="nerf_authors_v2">Sebastian Starke</a></div>
        <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="http://yutingye.info" target="_blank"
            class="nerf_authors_v2">Yuting Ye</a></div>
        <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="https://igl.ethz.ch/people/sorkine/"
            target="_blank" class="nerf_authors_v2">Olga Sorkine-Hornung</a></div>
      </div>
      <div>
        <span class="center">
          <video class="video-player" autoplay loop muted playsinline>
            <source src="./resources/video-teaser.webm">
            Your browser does not support the video tag.
          </video>
        </span>

      </div>
      <div class="link_column_nerf_v2 w-row">
        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
          <a href="./papers/walk-the-dog-camera-ready-with-supp.pdf" target="_blank" class="link-block w-inline-block">
            <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png"
              alt="paper" class="paper_img image-8 github_icon_nerf_v2" /></a>
        </div>
        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
          <a href="https://github.com/PeizhuoLi/walk-the-dog/" target="_blank" class="link-block w-inline-block">
            <img
              src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png"
              alt="paper" class="paper_img image-8 github_icon_nerf_v2" /></a>
        </div>
        <div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://drive.google.com/drive/folders/1-tqUTu4gAyFHyULCyIY7_AW_LoaquY7m"
            target="_blank" class="link-block w-inline-block">
            <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg"
              alt="paper" class="paper_img image-8_nerf nerf_db_icon" /></a>
        </div>
      </div>
      <div class="paper_code_nerf w-row">
        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
          <div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong></div>
        </div>
        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
          <div class="text-block-2">
            <strong class="bold-text-nerf_v2">Code</strong>
          </div>
        </div>
        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
          <div class="text-block-2"><strong class="bold-text-nerf_v2">Data and Model</strong></div>
        </div>
      </div>
      <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div>
    </div>
  </div>

  <div data-anchor="slide1" class="section nerf_section">
    <div class="grey_container w-container">
      <h2 class="grey-heading_nerf">Overview Video</h2>
      <div style="padding-top:56.17021276595745%" id="w-node-e5e45b1d55ac-81500a5f" class="w-embed-youtubevideo">
        <iframe
          src="https://www.youtube.com/embed/tNVO2jqeTNw?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0"
          frameBorder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto"
          allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>
  <div data-anchor="slide1" class="section nerf_section">
    <div class="w-container">
      <h2 class="grey-heading_nerf">Abstract &amp; Method</h2>

      <span class="center">
        <img src="./resources/teaser.png" class="full_figure" />
      </span>
      <p class="paragraph-3 nerf_text">
        We introduce a novel approach to learn a common phase manifold \(\mathcal{P}\) from datasets with drastically different skeletal structures without any supervision, using vector quantized periodic autoencoder. Each connected component in the manifold, visualized in a different color, is an ellipse embedded in high-dimensional space. Semantically similar motions from different characters are embedded into the same ellipse. The manifold is parametrized by:
        \[
        \Psi(\A, \phi) = \A^0\sin (2\pi\phi) + \A^1\cos (2\pi\phi)
        \]
      </p>

      <span class="center">
        <img src="./resources/arch.png" class="full_figure" />
      </span>
      <p class="paragraph-3 nerf_text">
        Starting with a short motion sequence \(\X \in \R^{J \times T}\),
        the encoder learns an intermediate representation using convolution.
        The representation is fed into the timing and the amplitude branch
        for predicting the phase \(\phi\), the frequency \(f\) and the
        amplitude \( \A \) of the pivot frame (rendered with mesh). A vector
        quantization (i.e. nearest neighbor search) is used in the amplitude
        branch to ensure the structure of the phase manifold. Note the
        codebook \( \mathcal{A}\) is shared among multiple VQ-PAEs. We
        calculate the embedding \(\P\) of the sequence assuming the
        frequency and amplitude stay constant in the sequence. The predicted
        phase manifold sequence is then passed through a convolutional
        decoder to reconstruct the input motion.
      </p>

      <span class="center">
        <img src="./resources/joint-training.png" class="overview_figure" />
      </span>
      <p class="paragraph-3 nerf_text">
        To align motions among different datasets, a common phase manifold 
        can be learned with a shared codebook \(\mathcal{A}\) and no additional supervision
        is required.
      </p>
    </div>
  </div>

  <div class="white_section_nerf">
    <div class="grey_container w-container">
      <h2 class="grey-heading_nerf">The Phase Manifold</h2>
      <p class="paragraph-3 nerf_text nerf_results_text">The learned manifold effectively clusters motions with the same semantic meaning into the same connected component:</p>

      <span class="center">
        <video class="video-player-small" autoplay loop muted playsinline>
          <source src="./resources/manifold.webm">
          Your browser does not support the video tag.
        </video>
      </span>

      <p class="paragraph-3 nerf_text nerf_results_text">When examing the average pose of point on the manifold, our method can also cluster raw motion captures from different characters into semantically similar groups, without any paired data or pre-defined joint mapping:</p>

      <span class="center">
        <video class="video-player-small" autoplay loop muted playsinline>
          <source src="./resources/average-pose.webm">
          Your browser does not support the video tag.
        </video>
      </span>

      <span class="center">
        <p class="paragraph-3 nerf_text nerf_results_text"></p>
      </span>
    </div>

    <div class="white_section_nerf">
      <div class="w-container">
        <h2 class="grey-heading_nerf">Applications</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">With the highly structured manifold, we can trivie motions from the same connected component but of different frequencies:</p>

        <span class="center">
          <video class="video-player-small" autoplay loop muted playsinline>
            <source src="./resources/retrival.webm">
            Your browser does not support the video tag.
          </video>
        </span>


        <p class="paragraph-3 nerf_text nerf_results_text">Using the embedding of the human motion as input, we can transfer it to the dog by running motion matching on the leanred manifold:</p>

        <span class="center">
          <video class="video-player-small" autoplay loop muted playsinline>
            <source src="./resources/transfer.webm">
            Your browser does not support the video tag.
          </video>
        </span>


        <p class="paragraph-3 nerf_text nerf_results_text">Since our latent space captures the core semantic meaning and alignment of motions, we can achieve character stylization immediately by transferring motions between datasets with different characteristic:</p>

        <span class="center">
          <video class="video-player-small" autoplay loop muted playsinline>
            <source src="./resources/characterization.webm">
            Your browser does not support the video tag.
          </video>
        </span>


        <p class="paragraph-3 nerf_text nerf_results_text">When multiple autoencoders are trained on multiple datasets together, a common phase manifold can be learned. Then, we can  transfer motions among various morphologies and characteristics at the same time:</p>

        <span class="center">
          <video class="video-player-small" autoplay loop muted playsinline>
            <source src="./resources/multiple.webm">
            Your browser does not support the video tag.
          </video>
        </span>
      </div>
    </div>

  </div>

  <div class="white_section_nerf">
    <div class="w-container">
      <h2 class="grey-heading_nerf">BibTeX</h2>
      <div class="grey_container w-container">
        <div class="bibtex">
          <pre><code>@inproceedings{Li2024walkthedog,
  title={WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds},
  author={Li, Peizhuo and Starke, Sebastian and Ye, Yuting and Sorkine-Hornung, Olga},
  booktitle = {SIGGRAPH, Technical Papers},
  year = {2024},
  doi={10.1145/3641519.3657508}
}
</code></pre>
        </div>
      </div>
    </div>
  </div>
</body>

</html>