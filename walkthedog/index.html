
<!DOCTYPE html><!-- Last Published: Fri Mar 27 2020 21:28:31 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="peizhuoli.github.io" data-wf-page="5e6fb768456f961381500a5f" data-wf-site="51e0d73d83d06baa7a00000f">
<head>
  <meta charset="utf-8"/>
  <title>WalkTheDog</title>
  <meta content="summary" name="twitter:card"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
  <link href="./motion_editing.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic","Changa One:400,400italic","Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Varela Round:400","Bungee Shade:regular","Roboto:300,regular,500"]  }});</script>
  <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="resources/logo.jpg" rel="shortcut icon" type="image/x-icon"/>
  <link href="resources/logo.jpg" rel="apple-touch-icon"/>
  <style>
    .video-player {
      margin-top: 50px;  /* Adds vertical space above the video */
      margin-bottom: 50px;  /* Adds vertical space below the video */
      width: 100%;    /* Sets the width of the video to 100% of its container's width */
      /* height: 100vh;  /* Sets the height of the video to 100% of the viewport height */
    }
  </style>

</style></head>
<body>
  <div class="section hero nerf-_v2">
    <div class="container-2 nerf_header_v2 w-container">
      <h1 class="nerf_title_v2">WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds</h1>
      <h1 class="nerf_subheader_v2">SIGGRAPH 2024</h1>
      <div class="nerf_authors_list_single w-row">
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="https://peizhuoli.github.io/" target="_blank" class="nerf_authors_v2">Peizhuo Li</a></div>
          <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6"><a href="https://www.sebastianxstarke.com/" target="_blank" class="nerf_authors_v2">Sebastian Starke</a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="http://yutingye.info" target="_blank" class="nerf_authors_v2">Yuting Ye</a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="https://igl.ethz.ch/people/sorkine/" target="_blank" class="nerf_authors_v2">Olga Sorkine-Hornung</a></div>
        </div>
                <div>
                  <span class="center">
                    <video class="video-player" autoplay loop muted controls>
                      <source src="./resources/video-teaser.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </span>
                  
                </div>
              <div class="link_column_nerf_v2 w-row">
                <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                  <a href="./papers/walk-the-dog-camera-ready-with-supp.pdf" target="_blank" class="link-block w-inline-block">
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
              </div>
              <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                <a href="https://github.com/PeizhuoLi/walk-the-dog/" target="_blank" class="link-block w-inline-block">
                <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
              </div>
                <div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://drive.google.com/drive/" target="_blank" class="link-block w-inline-block">
                  <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg" alt="paper" class="paper_img image-8_nerf nerf_db_icon"/></a>
                </div>
              </div>
                  <div class="paper_code_nerf w-row">
                    <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                      <div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong></div>
                    </div>
                      <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2">
                        <strong class="bold-text-nerf_v2">Code (Coming Soon)</strong></div>
                      </div>
                        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                          <div class="text-block-2"><strong class="bold-text-nerf_v2">Model (Coming Soon)</strong></div>
                        </div>
                      </div>
                          <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div></div>

                          <div data-anchor="slide1" class="section nerf_section">
                            <div class="w-container"><h2 class="grey-heading_nerf">Overview Video</h2>
                              <div style="padding-top:56.17021276595745%" id="w-node-e5e45b1d55ac-81500a5f" class="w-embed-youtubevideo stega_movie youtube">
                                <iframe src="https://www.youtube.com/embed/tNVO2jqeTNw?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameBorder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                              </div>
                              <!-- <p class="paragraph-3 nerf_text">Coming soon...</p> -->
                            </div>
                            </div>
                            <div data-anchor="slide1" class="section nerf_section">
                              <div class="grey_container w-container">
                                <h2 class="grey-heading_nerf">Abstract &amp; Method</h2>
                                <p class="paragraph-3 nerf_text">In this work, we present a manifold-aware transformer framework that can learn the dynamics of different garments driven by various unseen motions. Our network model the dynamics of a garment by exploiting its local interactions with the underlying human body, allowing us to handle different <i>garment types, body models, mesh resolutions and local connectivities</i>.
                                <span class="center">
                                <img src="./images/overview.png" class="overview_figure"/>
                                </span>

                                <p class="paragraph-3 nerf_text">
                                  We start with extracting the garment features and interaction features on the garment geometry from the past frames. Our manifold-aware transformer is then applied spatially to the input features and predicts the relative deformation gradients to the next frame. An initial prediction is obtained with a Poisson solver. After the collision refinement, we get the prediction for the next frame. We auto-regressively repeat this process until the desired number of frames is reached.</p>
                                <span class="center">                                
                                <img src="./images/method.png" class="overview_figure"/>
                                </span>
                            

                            <p class="paragraph-3 nerf_text">The <i><span style="color: rgb(252,184,46);">input features</span></i> are extracted from the faces of the input mesh. After being projected into <i><span style="color: rgb(247,188,214);">embedding space</span></i> by a linear transformation, they are fed into the transformer encoder. Our manifold-aware self-attention layers explicitly involve local connectivities of the input geometry, making it possible to predict accurate dynamics caused by seams. The output of the transformer encoder is projected to the output features by another linear transformation. The <i><span style="color: rgb(18,176,38);">output features</span></i> are then used to predict the next frame of garment deformation.
                            </p>

</div></div>

<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">Results</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Our method can predict the dynamics of different garments driven by various unseen motions.</p>
  <span class="center">
    <video width="800" height="450" autoplay loop muted controls>
      <source src="./resources/video-teaser.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </span>
  <p class="paragraph-3 nerf_text nerf_results_text">Due to the nature of the feature and network design, our network can easily handle the dynamics of garments on an unseen body model.</p>
  <div class="center">
    <span class="center">
      <img src="images/unseen_body.gif" class="skin_result">
    </span>

    <p class="paragraph-3 nerf_text nerf_results_text">The manifold-aware design also enables our network to handle topological cuts that cannot be reflected with only spatial features.</p>
  <div class="center">
    <span class="center">
      <img src="images/seam_cut.gif" class="skin_result">
    </span>
  </div>
</div></div>

<div class="white_section_nerf">
  <div class="grey_container w-container">
  <h2 class="grey-heading_nerf">Comparisons</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Here we compare our result to a supervised method. It can be seen that our method models the dynamics of a loose garment better than the baseline: </p>
  <span class="center"><img src="images/comparison_ssch.gif"></span>
  <p class="paragraph-3 nerf_text nerf_results_text">Here we compare our result to a generalizable method driven by graph convolution. It can be seen that our method generates consistent results, while the graph convolution based method suffers from unnatural artifacts due to high resolution but limited receptive field:</p>
  <span class="center"><img src="images/comparison_hood.gif"></span>
</div></div>

  </div>

<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">BibTeX</h2>
  <div class="grey_container w-container">
    <div class="bibtex">
          <pre><code>@inproceedings{Li2024walkthedog,
  title={WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds},
  author={Li, Peizhuo and Starke, Sebastian and Ye, Yuting and Sorkine-Hornung, Olga},
  booktitle = {SIGGRAPH, Technical Papers},
  year = {2024},
  doi={10.1145/3641519.3657508}
}
</code></pre>
      </div>
    </div>
    </div>
    </div>




<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.3057c11af.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]--></body></html>
