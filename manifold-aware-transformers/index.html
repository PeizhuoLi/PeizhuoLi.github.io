
<!DOCTYPE html><!-- Last Published: Fri Mar 27 2020 21:28:31 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="peizhuoli.github.io" data-wf-page="5e6fb768456f961381500a5f" data-wf-site="51e0d73d83d06baa7a00000f">
<head>
  <meta charset="utf-8"/>
  <title>Manifold-Aware Transformers</title>
  <meta content="A rigging framework that provides high-quality deformation." name="description"/>
  <meta content="Learning Skeletal Articulations with Neural Blend Shapes" property="og:title"/>
  <meta content="An end-to-end method for mesh character rigging" property="og:description"/>
  <meta content="summary" name="twitter:card"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
  <link href="./motion_editing.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic","Changa One:400,400italic","Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic","Varela Round:400","Bungee Shade:regular","Roboto:300,regular,500"]  }});</script>
  <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/logo.jpg" rel="shortcut icon" type="image/x-icon"/>
  <link href="images/logo.jpg" rel="apple-touch-icon"/>
  <style>
.wf-loading * {
    opacity: 0;
}
</style></head>
<body>
  <div class="section hero nerf-_v2">
    <div class="container-2 nerf_header_v2 w-container">
      <h1 class="nerf_title_v2">Neural Garment Dynamics via Manifold-Aware Transformers</h1>
      <h1 class="nerf_subheader_v2">Eurographics 2024</h1>
      <div class="nerf_authors_list_single w-row">
          <div class="w-col w-col-2 w-col-small-3 w-col-tiny-6"><a href="https://peizhuoli.github.io/" target="_blank" class="nerf_authors_v2">Peizhuo Li</a></div>
          <div class="w-col w-col-2 w-col-small-3 w-col-tiny-6"><a href="https://tuanfeng.github.io/" target="_blank" class="nerf_authors_v2">Tuanfeng Y. Wang</a></div>
          <div class="w-col w-col-3 w-col-small-3 w-col-tiny-6"><a href="mailto:tkesdogan@student.ethz.ch" target="_blank" class="nerf_authors_v2">Timur Levent Kesdogan</a></div>
          <div class="w-col w-col-2 w-col-small-3 w-col-tiny-6"><a href="https://www.duygu-ceylan.com/" target="_blank" class="nerf_authors_v2">Duygu Ceylan</a></div>
          <div class="w-col w-col-2 w-col-small-3 w-col-tiny-6"><a href="https://igl.ethz.ch/people/sorkine/" target="_blank" class="nerf_authors_v2">Olga Sorkine-Hornung</a></div>
        </div>
          <!-- <div class="columns-6 w-row"><div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4">
            <div class="nerf_mobile_inst"><span class="text-span_nerf">1 </span>Beijing </div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">2</span>Peking University</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">3</span>Hebrew University</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">4</span>ETH Zurich</div></div>
            <div class="nerf_mobile_col_inst w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="nerf_mobile_inst"><span class="text-span_nerf">5</span>Tel-Aviv University</div></div>
          </div>
            <div class="nerf_authors_list_single nerf_authors_affiliation w-row">
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Beijing Film Academy</h1></div>
              <div class="column w-col w-col-2"><h1 class="nerf_affiliation_v2">Peking University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Hebrew University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">ETH Zurich</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Tel-Aviv University</h1></div>
              <div class="w-col w-col-2"><h1 class="nerf_affiliation_v2">Peking University</h1></div>
            </div> -->
                <div>
                  <span class="center"><img src="images/video_teaser.gif"></span>
                </div>
              <div class="link_column_nerf_v2 w-row">
                <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                  <a href="./papers/manifold-aware-transformers-camera-ready.pdf" target="_blank" class="link-block w-inline-block">
                    <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
                <!-- <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png" alt="paper" srcset="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png 500w, https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01.png 672w" sizes="(max-width: 479px) 12vw, (max-width: 767px) 7vw, (max-width: 991px) 41.84375px, 56.6875px" class="paper_img image-8_nerf"/></a> -->
              </div>
              <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                <a href="https://github.com/PeizhuoLi/manifold-aware-transformers" target="_blank" class="link-block w-inline-block">
                <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cae3b53b42ebb3dd4175a82_68747470733a2f2f7777772e69636f6e66696e6465722e636f6d2f646174612f69636f6e732f6f637469636f6e732f313032342f6d61726b2d6769746875622d3235362e706e67.png" alt="paper" class="paper_img image-8 github_icon_nerf_v2"/></a>
              </div>
                <div class="column-2 w-col w-col-4 w-col-small-4 w-col-tiny-4"><a href="https://drive.google.com/drive/folders/1tjvFg6ymHUyjxpKmVs_U8ysBhK6ILEA_?usp=share_link" target="_blank" class="link-block w-inline-block">
                  <img src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7136849ee3b0a0c6a95151_database.svg" alt="paper" class="paper_img image-8_nerf nerf_db_icon"/></a>
                </div>
              </div>
                  <div class="paper_code_nerf w-row">
                    <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                      <div class="text-block-2"><strong class="bold-text-nerf_v2">Paper</strong></div>
                    </div>
                      <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4"><div class="text-block-2">
                        <strong class="bold-text-nerf_v2">Code</strong></div>
                      </div>
                        <div class="w-col w-col-4 w-col-small-4 w-col-tiny-4">
                          <div class="text-block-2"><strong class="bold-text-nerf_v2">Model</strong></div>
                        </div>
                      </div>
                          <div class="nerf_slide_nav w-slider-nav w-slider-nav-invert w-round"></div></div></div>

                          <div data-anchor="slide1" class="section nerf_section">
                            <div class="w-container"><h2 class="grey-heading_nerf">Overview Video</h2>
                              <div style="padding-top:56.17021276595745%" id="w-node-e5e45b1d55ac-81500a5f" class="w-embed-youtubevideo stega_movie youtube">
                                <iframe src="https://www.youtube.com/embed/v6FCTHmjyqI?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameBorder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                              </div>
                              <!-- <p class="paragraph-3 nerf_text">Coming soon...</p> -->
                            </div>
                            </div>
                            <div data-anchor="slide1" class="section nerf_section">
                              <div class="grey_container w-container">
                                <h2 class="grey-heading_nerf">Abstract &amp; Method</h2>
                                <p class="paragraph-3 nerf_text">In this work, we present a manifold-aware transformer framework that can learn the dynamics of different garments driven by various unseen motions. Our network model the dynamics of a garment by exploiting its local interactions with the underlying human body, allowing us to handle different <i>garment types, body models, mesh resolutions and local connectivities</i>.
                                <span class="center">
                                <img src="./images/overview.png" class="overview_figure"/>
                                </span>

                                <p class="paragraph-3 nerf_text">
                                  We start with extracting the garment features and interaction features on the garment geometry from the past frames. Our manifold-aware transformer is then applied spatially to the input features and predicts the relative deformation gradients to the next frame. An initial prediction is obtained with a Poisson solver. After the collision refinement, we get the prediction for the next frame. We auto-regressively repeat this process until the desired number of frames is reached.</p>
                                <span class="center">                                
                                <img src="./images/method.png" class="overview_figure"/>
                                </span>
                            

                            <p class="paragraph-3 nerf_text">The <i><span style="color: rgb(252,184,46);">input features</span></i> are extracted from the faces of the input mesh. After being projected into <i><span style="color: rgb(247,188,214);">embedding space</span></i> by a linear transformation, they are fed into the transformer encoder. Our manifold-aware self-attention layers explicitly involve local connectivities of the input geometry, making it possible to predict accurate dynamics caused by seams. The output of the transformer encoder is projected to the output features by another linear transformation. The <i><span style="color: rgb(18,176,38);">output features</span></i> are then used to predict the next frame of garment deformation.
                            </p>

</div></div>

<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">Results</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Our method can predict the dynamics of different garments driven by various unseen motions.</p>
  <span class="center">
    <img class="skin_result" src="images/video_teaser.gif">
  </span>
  <p class="paragraph-3 nerf_text nerf_results_text">Due to the nature of the feature and network design, our network can easily handle the dynamics of garments on an unseen body model.</p>
  <div class="center">
    <span class="center">
      <img src="images/unseen_body.gif" class="skin_result">
    </span>

    <p class="paragraph-3 nerf_text nerf_results_text">The manifold-aware design also enables our network to handle topological cuts that cannot be reflected with only spatial features.</p>
  <div class="center">
    <span class="center">
      <img src="images/seam_cut.gif" class="skin_result">
    </span>
  </div>
</div></div>

<div class="white_section_nerf">
  <div class="grey_container w-container">
  <h2 class="grey-heading_nerf">Comparisons</h2>
  <p class="paragraph-3 nerf_text nerf_results_text">Here we compare our result to a supervised method. It can be seen that our method models the dynamics of a loose garment better than the baseline: </p>
  <span class="center"><img src="images/comparison_ssch.gif"></span>
  <p class="paragraph-3 nerf_text nerf_results_text">Here we compare our result to a generalizable method driven by graph convolution. It can be seen that our method generates consistent results, while the graph convolution based method suffers from unnatural artifacts due to high resolution but limited receptive field:</p>
  <span class="center"><img src="images/comparison_hood.gif"></span>
</div></div>

  </div>

<div class="white_section_nerf">
  <div class="w-container">
  <h2 class="grey-heading_nerf">BibTeX</h2>
  <div class="grey_container w-container">
    <div class="bibtex">
          <pre><code>@article{Li2024NeuralGarmentDynamics,
   author    = {Li, Peizhuo and Wang, Tuanfeng Y. and Kesdogan, Timur Levent and Ceylan, Duygu and Sorkine-Hornung, Olga},
   title     = {Neural Garment Dynamics via Manifold-Aware Transformers},
   journal   = {Computer Graphics Forum (Proceedings of EUROGRAPHICS 2024)},
   volume    = {43},
   number    = {2},
   year      = {2024},
}
</code></pre>
      </div>
    </div>
    </div>
    </div>




<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.3057c11af.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]--></body></html>
