[{"authors":["admin"],"categories":null,"content":"My name is Peizhuo Li (李沛卓). I am a direct doctorate student at Interactive Geometry Lab under the supervision of Prof. Olga Sorkine-Hornung. My research interest lies in the intersection between deep learning and computer graphics. In particular, I am interested in practical problems related to character animation. Prior to my PhD study, I was an intern at Visual Computing and Learning lab at Peking University and advised by Prof. Baoquan Chen.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://peizhuoli.github.io/author/peizhuo-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peizhuo-li/","section":"authors","summary":"My name is Peizhuo Li (李沛卓). I am a direct doctorate student at Interactive Geometry Lab under the supervision of Prof. Olga Sorkine-Hornung. My research","tags":null,"title":"Peizhuo Li","type":"authors"},{"authors":["Weiyu Li*","Xuelin Chen*","Peizhuo Li","Olga Sorkine-Hornung","Baoquan Chen"],"categories":null,"content":"","date":1690848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690848000,"objectID":"47ed83fcdeb151fc079638b7f1b0fa8f","permalink":"https://peizhuoli.github.io/publication/genmm/","publishdate":"1970-01-01T01:33:43+01:00","relpermalink":"/publication/genmm/","section":"publication","summary":"We present Generative Motion Matching (GenMM), a generative model that \"mines\" as many diverse motions as possible from a single or few example sequences. GenMM is training-free and can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures.","tags":null,"title":"Example-based Motion Synthesis via Generative Motion Matching","type":"publication"},{"authors":["Sigal Raab","Inbal Leibovitch","Peizhuo Li","Kfir Aberman","Olga Sorkine-Hornung","Daniel Cohen-Or"],"categories":null,"content":"","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"b967c89cb25aabddc0f0b87d31895854","permalink":"https://peizhuoli.github.io/publication/modi/","publishdate":"1970-01-01T01:33:43+01:00","relpermalink":"/publication/modi/","section":"publication","summary":"The emergence of neural networks revolutionized motion synthesis, yet synthesizing diverse motions remains challenging. We present MoDi, an unsupervised generative model trained on a diverse, unstructured, unlabeled dataset, capable of synthesizing high-quality, diverse motions. Despite dataset's lack of structure, MoDi yields a structured latent space for semantic clustering, enabling applications like semantic editing and crowd simulation. We also introduce an encoder that inverts real motions into MoDi's motion manifold, addressing ill-posed challenges like completion from prefix and spatial editing, achieving state-of-the-art results surpassing recent techniques.","tags":null,"title":"MoDi: Unconditional Motion Synthesis from Diverse Data","type":"publication"},{"authors":["Peizhuo Li","Kfir Aberman","Zihan Zhang","Rana Hanocka","Olga Sorkine-Hornung"],"categories":null,"content":"","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"795dad20da360b39e1ae58e91d14ebd4","permalink":"https://peizhuoli.github.io/publication/ganimator/","publishdate":"1970-01-01T01:33:42+01:00","relpermalink":"/publication/ganimator/","section":"publication","summary":"We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. It also enables applications including crowd simulation, key-frame editing, style transfer, and interactive control for a variety of skeletal structures e.g., bipeds, quadropeds, hexapeds, and more, all from a single input sequence.","tags":null,"title":"GANimator: Neural Motion Synthesis from a Single Sequence","type":"publication"},{"authors":["Peizhuo Li","Kfir Aberman","Rana Hanocka","Libin Liu","Olga Sorkine-Hornung","Baoquan Chen"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"feb669f16066b57422922ebc97944d11","permalink":"https://peizhuoli.github.io/publication/neural-blend-shapes/","publishdate":"1970-01-01T01:33:41+01:00","relpermalink":"/publication/neural-blend-shapes/","section":"publication","summary":"We develop a neural technique for articulating 3D characters using enveloping with a pre-defined skeletal structure, which is essential for animating character with motion capture (mocap) data. Furthermore, we propose neural blend shapes -- a set of corrective pose-dependent shapes that is used to address the notorious artifacts caused by standard rigging and skinning technique in joint region.","tags":null,"title":"Learning Skeletal Articulations with Neural Blend Shapes","type":"publication"},{"authors":["Kfir Aberman*","Peizhuo Li*","Dani Lischinski","Olga Sorkine-Hornung","Daniel Cohen-Or","Baoquan Chen"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"11d7d92fd6b3c136219fb16a8141ae6b","permalink":"https://peizhuoli.github.io/publication/skeleton-aware/","publishdate":"1970-01-01T01:33:40+01:00","relpermalink":"/publication/skeleton-aware/","section":"publication","summary":"We introduce a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs. Importantly, our approach learns how to retarget without requiring any explicit pairing between the motions in the training set.","tags":null,"title":"Skeleton-Aware Networks for Deep Motion Retargeting","type":"publication"}]